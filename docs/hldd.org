#+TITLE: Spread: High-Level Technical Design
#+AUTHOR: F. Bielejec
#+EMAIL: fbielejec@gmail.com
#+TEXINFO_PRINTED_TITLE: Spread: High-Level Technical Design
#+SUBTITLE: version {{{version}}}, {{{updated}}}
#+OPTIONS: ':t toc:t author:t email:t
#+LANGUAGE: en
#+STARTUP: overview

* DONE Introduction

Software package for visualizing phylogeographic reconstructions resulting from Bayesian inference of viral diffusion processes called SPREAD (Spatial Phylogenetic Reconstruction of Evolutionary Dynamics) has been first developed and published in 2011 (see [[https://academic.oup.com/bioinformatics/article/27/20/2910/202157][Bioinformatics]]) and since seen one major version upgrade in 2016 (see [[https://academic.oup.com/mbe/article/33/8/2167/2579258][Molecular Biology and Evolution]]).

This particular program has proven to be quite popular, mainly due to the unprecedented quantity in which the viral data and accompanying meta-data is now availiable.
Making sense of these data is virtually impossible without the proper context, i.e. projecting the findings in the geographic space.

This document describes the proposed conceptual design of a system meant to replace the existing legacy software.
It's target audience is the stakeholders, UI/UX designers and developers of the new system.
This document provides a framework for a more detailed requirements and design activities in later phases of the project, and is subject to change.

* DONE Current System
Most recent version of Spread in use today is a classic desktop application, packaged and distributed as an executable Java JAR.
The home tab of the user interface allows for parsing 4 distinct types of input.

#+CAPTION: Tab for parsing discrete traits tree
file:home_tab.png

This software package is a part of larger group of related programs jointly knows as [[https://github.com/beast-dev/beast-mcmc][BEAST]].
It provides visualization and summarizing capabilities by projecting the findings in geographical coordinates.

# It produces outputs in
A detailed tutorial, along with more screenshots is hosted [[https://rega.kuleuven.be/cev/ecv/software/SpreaD3_tutorial][here]].
The details of algorithms and procedures for producing said projections can be found in the [[https://lirias.kuleuven.be/1956532?limo%3D0][Section 5.3 of Thesis]].

** DONE Functional Description <<previous_functional_desc>>
[[https://rega.kuleuven.be/cev/ecv/software/SpreaD3][SpreaD3]] is an application to analyze and visualize pathogen phylodynamic reconstructions resulting from Bayesian inference of sequence and trait evolutionary processes.

As such it places itself at the end of a file-mediated pipeline and is geared towards analyzing and displaying the outputs of analysis from a popular Bayesian phylogenetic inference software [[https://github.com/beast-dev/beast-mcmc][BEAST]].
An overview and documentation of the the BEAST software package can be found [[http://beast.community/index.html][here]].

# In theory it can also accommodate input generated by other phylogenetic inference tools, as long as the nodes and branches of the trees are annotated using the compatible syntax.

Below diagram presents a general context in which SpreaD3 operates:

#+begin_src plantuml :file functional.png
package "BEAST inputs" {
  file "Genetic data" as genetic
  file "Temporal data" as temporal
  file "Other metadata" as meta
}

package "SPREAD inputs" {
  file "MCC tree with discrete traits" as discreteTree
  file "MCC tree with continuous traits" as continuousTree
  file "tree distribution with continuous traits" as continuousTrees
  file "log file from BSSVS analysis" as bssvs
}

genetic -down-> [BEAST]
temporal -down-> [BEAST]
meta -down-> [BEAST]

[BEAST] -down-> discreteTree
[BEAST] -down-> continuousTree
[BEAST] -down-> continuousTrees
[BEAST] -down-> bssvs

discreteTree -down-> [SPREAD] : geographical coordinates
continuousTree -down-> [SPREAD]
continuousTrees -down-> [SPREAD]
bssvs -down-> [SPREAD]

[SPREAD] ..> Visualisation : geojson map data

#+end_src
#+RESULTS:
[[file:functional.png]]

When analyzing trees annotated with the discrete data Spread needs additional inputs in the form of delimited text-files matching discrete traits with the geographical coordinates, typicaly it also needs user-supplied map data in the form of [[https://geojson.org/][geoJSON]] files.

The final visualisation comes in a form of an interactive map, with the data overlayed on top of it, where the user has control over various aspects of color coding, the attributes associated with each map component and color choices.

** DONE User Community Description
Primary users of the Spread software are researchers in the fields of epidemiology, virology, phylogenetics and bioinformatics.
The visualisations stemming frm SPREAD can be used to inform medical workers, health care officials, decision-makers etc.
** DONE Technical Architecture <<previous_tech_arch>>
In this section we identify and describe the architecture of the current system.

# What type of processing is the current system responsible for?
Spreads main responsibility is parsing and processing of BEAST output files:
- Summary tree files with discrete or continuously annotated nodes
- Files with distribution of trees with continuous trait annotations
- Log files containing a posterior distribution of rate indicators from a Bayesian stochastic search variable selection procedure.

The information found in those files, combined with a geographical coordinates data (in the case of discretely annotated trees) is used to parse and represent the hierarchical tree structures as primitives (such as points or lines) on a map.
These primitives will typically have associated geographical (latitide / longitude coordinates), temporal (timestamps) and various other meta-data associated with them.
Parsing step produces a JSON file with a following schema:

#+begin_src plantuml :file json_schema.png
class "root" as root {
    .. Objects ..
    properties: [object Object]
}
class "properties" as root.properties {
    .. Objects ..
    timeLine: [object Object]
    axis:  [object Object]
    lineAttributes: [array [object Object]]
    pointAttributes: [array [object Object]]
    areaAttributes: [array [object Object]]
    layers: [array [object Object]]
}
class "timeLine" as root.properties.timeLine {
    .. Properties ..
    startTime: date
    endTime: date
}
class "axis" as root.properties.axis {
    .. Properties ..
    xCoordinate: string
    yCoordinate: string
}
class "line / point / area attributes" as root.properties.lineAttributes {
    .. Properties ..
    type: array
    .. Objects ..
    attribute: [object Object]
}

note left of root.properties.lineAttributes.attribute : Discrete attributes have a domain field listing all unique values \n Continuous have a range [min, max].

class "attribute" as root.properties.lineAttributes.attribute {
    .. Properties ..
    id: string
    scale: enum
    range : array[2]
    domain: array[n]
}

class "layers" as root.properties.layers {
    .. Properties ..
    type: array
    .. Objects ..
    geoJSONLayer: [object Object]
    treeLayer: [object Object]
}

class "tree" as root.properties.layers.tree {
    .. Properties ..
    id: string
    type: string
    description: string
    points: [array [object Object]]
    lines: [array [object Object]]
    areas: [array [object Object]]
}

class "points" as root.properties.layers.tree.points {
    .. Properties ..
    type: array
    .. Objects ..
    point: [object Object]
}

class "point" as root.properties.layers.tree.points.point {
    .. Properties ..
    id: number
    coordinate: [object Object]
    startTime: date
    attributes: [object Object]
}

class "lines" as root.properties.layers.tree.lines {
    .. Properties ..
    type: array
    .. Objects ..
    line: [object Object]
}

class "line" as root.properties.layers.tree.lines.line {
    .. Properties ..
    id: number
    startPointId: number
    endPointId:	number
    startTime: date
    endTime: date
    attributes: [object Object]
}

root -- root.properties
root.properties -- root.properties.timeLine
root.properties -- root.properties.axis
root.properties -- root.properties.lineAttributes
root.properties.lineAttributes -- root.properties.lineAttributes.attribute
root.properties -- root.properties.layers
root.properties.layers -- root.properties.layers.tree

root.properties.layers.tree -- root.properties.layers.tree.points
root.properties.layers.tree.points -- root.properties.layers.tree.points.point

root.properties.layers.tree -- root.properties.layers.tree.lines
root.properties.layers.tree.lines -- root.properties.layers.tree.lines.line

root.properties.layers.tree.points.point <|- root.properties.layers.tree.lines.line : two pointers

root.properties.lineAttributes.attribute <|- root.properties.layers.tree.points.point
root.properties.layers.tree.lines.line -|> root.properties.lineAttributes.attribute
#+end_src
#+RESULTS:
[[file:json_schema.png]]

---
*NOTE*

An exmaple of a data-set that can be analyzed and visualised by Spread: [[https://github.com/fbielejec/SPREAD/blob/master/docs/236_subG_PT_cauchy_geo.mcc.tre][MCC tree file with continuous annotations]].
The output generated from the program after parsing this data can be found [[https://github.com/fbielejec/SPREAD/blob/master/docs/spread_data_example.json][here]].
For brevity it does not include the geoJSON layer which creates the map on which the estimates are displayed.
A minimal subset of this data, containing a single branch joining two nodes and their corresponding meta-data (attributes) can be found [[https://github.com/fbielejec/SPREAD/blob/master/docs/spread_data_example_minimal.json][here]].

---

Such JSON file is than loaded into the program once again to produce a visualisation:

#+begin_src plantuml :file subsystems.png
state Input {
  Discrete : summary tree file
  Discrete : BSSVS log file

  Discrete --> GeographicalCoordinates : combine
  GeographicalCoordinates : file with a mapping from trait name to its geographical coordinates

  Continuous : summary tree file
  Continuous : trees distribution file
}

Input --> JSON : parse data
JSON : file with geoJSON layer and data primitives for plotting

JSON --> VisualisationEngine : load file
VisualisationEngine : uses D3 libraries for plotting parsed data
#+end_src
#+RESULTS:
[[file:subsystems.png]]

The visualization is a stand-alone HTML document which user opens in the browser, gaining interactive control over different visualization components.
It can be controlled by a time slider, and tree projections over time can be animated, paused, fast-forwarded, or re-winded.
Color settings can be based on the attributes associated with each component and filled using selected color-palettes

# What are the major application components?
We can divide the application into three major components:
- parsing engine, capable of summarizing various inputs and combining them with external information, vanilla Java codebase.
- graphical user interface, written in the Swing framework.
- JavaScript visualization engine, which uses D3 library for rendering and creates a html + JS output in a user-specified location.

Spread is a desktop application, relying on end-user operating system for data storage, thread management etc.
The parsing engine and the graphical user-interface are both written in Java, with the visualization engine using a set of JavaScript libraries to create essentially a static web page which can be (locally) opened in the users browser.

* DONE Goals, Objectives, and Rationale for New or Significantly Modified System <<rationale>>

The most-recent version Spread (SemVer 0.9.7) was released in year 2016.
Since than it has attracted many users, and although individual downloads were not tracked, the joint number of citations with an earlier version of the software package is well over 600.
This highlights a need for user-friendly tool for visual display of pathogen dispersal.

At the same time not only is it a significant time-span for any software system to go without major maintenance, but a majority of the design and architecture was simply carried from the earlier version.
Below we higlight major shortcoming and ills plaguing the current version of Spread.

=desktop application=
All previous versions of Spread were a classic GUI desktop applications, installed on a personal or work computers.
They relied on the user Operating System to store, retrieve and analyze data.
Major shortcoming was the inability to easily retrieve and edit previous analyses, especially between different workstations.

It also hindered the development, as the major prevalent Jave Runtime Environment (JRE) installed across desktop computers at that time was version 6, making it impossible to use modern features of the programming language.
With a new 6 monthly [[https://www.oracle.com/java/technologies/java-se-support-roadmap.html][release cycle]] introduced recently by Oracle, it would be all the harder to push the burden of updating the JRE to the end-user.
In our opinion this necessitates a move to a classic server / client architecture, where the developers control the updates, Runtime Environment, data storage and other aspects of the development, in a manner that is transparent to the user, yet lifts all these usability constraints.

=data persistance=
This point ties to the previous one, yet due to it's importance it is discussed separately.
As already mentioned desktop version of Spread relied solely on the end user to store the inputs, outputs and results of the analysis.
It made it also her responsibility to move the data between different workstation, and maintain the file structure to be revisited should he want to re-analyse the data.
All of these concerns can be moved to the software itself with a use of Relational Database for storage.

In the previous software versions the generated visualisations came in a form of a static website created in a singel directory on the end-users computer.
The rendering step would simply bundle together the generated JSON data, the bundled JavaScript D3 [[https://github.com/phylogeography/d3-renderer][plotting scripts]] and the HTML entry-point.
It made it user responsibility to create and host this website or view it locally, by opening the index page in the browser.
In recent years many browsers stopped supporting accessing local data files, for security reasons. and users had to resort to using cumbersome command-line arguments to turn browsers unsafe features on.
By creating a classic client-server architecture we can use object-based storage architectures such as [[https://aws.amazon.com/s3/][Amazon S3]] or [[https://ipfs.io/][IPFS]] for hosting created websites.

=usability=
One of the major user feedbacks was the inconvenience of the two-step analysis of the data.
User would load the initial data, manipulate the settings and generate an internal representation in a form of a JSON file (see [[previous_tech_arch][Technical Architecture]]).
This file had to be than loaded into the program again to generate the visualization, when in fact this step simply created a directory with the files bundled together.
This was driven by the idea that users might want to combine different data-sets, by merging these JSON files together.

#+CAPTION: merging data in the previous version of SPREAD
file:merge_tab.png

In practice this turned out to be of marginal importance for the users.
New system should simplify and streamline the process of obtaining a visualization, within a minimal number of steps needed.

=use of D3.js library=
Even today D3.js is still a great way for creating one-off visualizations on the web.
However it makes a poor fit with modern web application frameworks, directly overlapping with how these frameworks manipulate the browsers DOM.
It is also a fairly low-level library, providing mainly graph primitives and not offering any built-in capabilities for working with maps and geo-data.
The modfied system should utilize a library with an API directly aimed at working with maps to produce the visualizations.

** DONE Project Purpose

The magnitude of these changes deems it necessary to replace the existing system with a new one.
Large parts of the codebase, providing the parsing and analysis capabilities can be re-used, and wrapped as a web-server with API endpoints for interacting with the briwser client application [[goals_and_objectives][(see System Goals and Objectives]]).

** DONE System Goals and Objectives <<goals_and_objectives>>
# Briefly describe the goals and objectives of the new or modified system. Clearly state the business and/or operational problem that will be solved.

New system ought to provide a functional, user-friendly web-based tool that will serve as successor to the [[https://rega.kuleuven.be/cev/ecv/software/spread][SPREAD software]] to visualize Bayesian phylogeographic estimates.
The tool should be able to load both discrete and continuous phylogeographic estimates produced by BEAST and interactively visualize them as projections on geographic maps, based on the annotated and user-provided information.

It will replace the existing system and allevite all of the problems plaguing it, namely the data persistance problems, the usability issues and the problems with sharing of the produced visualisations.
It will provide ways for users to manage, store and revisit their data and visualisations

** DONE Proposed System
# Instructions: Provide a succinct description of the proposed system. Sections 5 and 6 will describe the proposed system in more detail.
*** DONE System Scope
Here we outline the responsibilities and boundaries of the proposed system.

=parsing of BEAST produced inputs=
This version of Spread should be capable of processing the following inputs:
- Summary tree files with discrete annotations
- Summary tree files with continuous annotations
- Files with distribution of trees with continuous trait annotations
- Log files containing a posterior distribution of rate indicators from a Bayesian stochastic search variable selection procedure.

=user management=
Another responsibility of the system is to maintains user sessions.
Specifically software will handle
- email based (i.e. magic links) login and sign-on on multiple devices
- session and management (cookie based)

=data persistance=
Data persistance for every user's account means storing:
- BEAST input files per analysis
- settings used to parse those files
- resulting visualisations, with the ability to share them (through URLs)

=visualisations=
The end-product of the software will be the map-based interactive visualisations.
They should maintain have the following features:
- interactive, with time based animation
- overlayed on maps
- zoom-in and zoom-out on the details
- interactive /detail-on-demand/: select and highlight taxa (based on string content) and locations
- ability to hide elements of visualisation: nodes, branches, polygons, map elements etc
- export to svg graphics

*** DONE Business Processes Supported

Below diagram is a high-level overview of the supported processes.

#+begin_src plantuml :file business_process.png
(*) --> if "user authenticated?" then
  -->[true] "show user home page" as authed
else
  -->[false] "send email with magic link"
  --> "open link"
  --> authed

authed --> "new analysis" as new
--> "import data"
--> "set parsing settings" as settings
--> "parse data and generate visualisation" as output

authed --> "edit previous analysis" as edit

edit --> "load new data"
--> settings

edit --> "edit parsing settings"
--> output
#+end_src
#+RESULTS:
[[file:business_process.png]]

They can generally be divided into a process of user login and authentication and the process of analysing and visualizing the data.

*** DONE High-Level Functional Requirements

General user-interface requirements:

- A minimal number of steps to obtain a good quality visualization
- Animated visualization of phylogenies projected on maps (with the ability to freeze and export)
- The ability to select and highlight taxa (based on string content) and locations
- Custom coloring and styling
- The ability to zoom in on parts of the projection
- Good export capabilities (vector-based graphics)
- Ensure browser compatibility with popular browsers
- The ability to retrieve and edit previous analyses
- Sharing of analyses through URLs
- User authentication and management

*** DONE Summary of Changes
# Instructions: If changing an existing system, briefly summarize the changes that this project will make to the system (e.g., functionality changes, technology changes, environment changes.

The majority of the changes will be focused on creating a client-server architecture.

The new system, although requires substantial changes that warrant a new code-base, will be able to re-use some parts of the previous releases.
Specifically the numerical methods and algorithms responsible for the parsing of the tree files as well as computing the various statistics can be used with the new application, providing it also uses JVM as it's runtime environment.

# Depending on the exact programming language chosen

User authentication and management, webserver endpoints for interacting with the application as well as Object and Relational storage will have to be developed.
The visualization engine, responsible for displaying the analyzed data will also be developed anew, with a different set of technologies.

The deployment environment will be changed from a desktop-based application to a server - client architecture.
The exact infrastructure will most probably be coming form a cloud provider, with instances of a Compute Cloud for hosting the server and the client server to the users browsers, RDS for Relational storage and S3 or similar solution used for the object storage.

* DONE Factors Influencing Technical Design
# Instructions: This section describes the standards, assumptions, and constraints that influence the technical design of the proposed system.
** DONE Assumptions and Dependencies
# Instructions: Describe any assumptions or dependencies regarding the system and its use.

Due to the specialized nature of the system, we do not expect the application to be subject to a significant network traffic or needing to scale horizontally over time.
Nonetheless the size of the files and the associated meta-data used in the application can be quite significant, and the architecture needs to take into account the requirement of uploading these large files in http requests as well as storing them over-time.

The client part of the system, runing in the browser environment will most likely be viewed only on large screen sizes, which can influence the UI/UX design, i.e. there is no requirement for displaying those on tablets or mobile devices.

After the initial phase of development we expect the system to be complete and functional in a way that allows it to be used for at least next coupel of years without significant maintenance required, which is in line with how most scientific software is being used today.
This means limiting the number of components making the system, using cloud providers and automation whenever possible.
The system should be relatively low-cost over time, although because it enters the public domain and will be used solely for research, we can search for solution alleviating at least parts of the running costs (see [[https://aws.amazon.com/research-credits/][AWS Cloud Credits for Research]]).

** DONE Constraints

Chosing the server-client over a previous desktop based architecture lifts many of the constraints of the legacy system, previous mentioned in [[rationale][Goals, Objectives, and Rationale for New or Significantly Modified System]].
The client part of the application will consist of static content running in the end-users browser environment, therefore simply needs to meet a standard set of requirements for a browser-based application and be inter-operable with modern browsers and typical hardware environment of a desktop or laptop PC.
This part of the system will need to be written from the ground up and the works can be divided into two sub-parts:

1. The interface responsible for parsing an analyzing the data.
2. The visualization engine, responsible for the display of the analyzed data.

There are no restrictions on the programming language or specific frameworks / libraries to build them although typically JavaScript and languages with JS as their compilation target are the natural choices when building Web-based user intefaces.
Server part of the system should favour code re-use as much as it is practically possible, favouring languages with JVM as their runtime, interoperable with a Java 6 typ of codebase.
This does not preclude a hybrid-codebase solution, as long as these requiremenets are met.

The storage tier solution should be chosen to support storing and retrieving potentially large BEAST output files, as described by the [[previous_functional_desc][Functional description]] of the legacy system.
The uploaded resources should remain availiable over-time and therefore the Object storage solution which presents high availiability is prefferable.
The database tier should be well suited for querying both the user session management data, as well as the results of parsing and analyzing these files (see [[previous_tech_arch][Technical Architecture]] of the legacy system), which wil most likely remain object-based in the new system as well.
Login and authorization process should be streamlined and favour usability rather than privacy.
Allthough the system definitely doesn not need to maintain a high uptime and availiability, basic monitoring and error logging solution should be put in place, to facilitate bugfixing and post-mortems.

** DONE Design Goals

The technical design should be guided by these principles:

- the storage solutions should minimize the long term running costs and be suitable for storing and efficiently querying object-based data
- the server tier of the application should favour code re-use
- interface should maintain minimal number of steps for obtaining a visualization.
- the part of the interface which displays the visualizations should be customizable and interacitve, to support a vast amount of meta-data that these analysis can contain.

* DONE Proposed System
** DONE High-Level Operational Requirements and Characteristics
# *** User Community Description
# *** Non-Functional Requirements

=User Community Description=
We expect users to be coming from different geographical IP locations, and uses the systems in longer sessions, but spread out over large periods of time.
We do not expect more than 20-30 users to be logged within the system concurrently.

=Security and Privacy=
The system will be accessed via the Internet, and although we do not consider any of the stored data as highly critical, it should deploy measure protecting it from the most common vulnerabilities, as per secure coding guidelines such as the [[https://owasp.org/www-project-proactive-controls/][Open Web Application Security Project]] (OWASP) guidelines.
Most of the data in the system will be accessed from the authorized user sessions, however as per requirements the visualizations will be distributed outside of the CMS to anyone holding a URL link.

=Availability=
The anticipated uptime for the service is 24/7, yet some downtime of up to due to maintainance or external circumstances is acceptable for the system.
There are no strict requirements for how quickly should the system come back up after an outage, yet we can specify a 48H time window as the maximal accepted length of an outage.

=Volume and Performance Expectations=
We expect no more than 30 users transacting with the system at any given point in time, with the anticipated traffic spread out uniformly, without any expected peaks of processing in specific time periods.
Average transaction size should be moderate, and limited to a single upload transmission, albeit of a potentially large data-file (see [[previous_functional_desc][Functional description]] of the legacy system).

** DONE High-Level Architecture

# Instructions: Provide a high-level overview of how system functionality will be allocated to logical subsystems or components. This section should not go into the detail that the SDD deliverable will cover later in the project. Rather, this section should identify the logical user groups, application components, data components, and interfacing systems. Illustrate the collaboration and interaction between the major components. Identify any relevant design patterns or reuse relevant to the design.
# Insert diagram here.
# In a table similar to the one below, identify the alternatives considered for the overall architecture. For example, discuss any decision making around building a brand new system versus enhancing an existing system. This alternatives discussion is intended to differentiate between the fundamental options for designing a technical solution. If more detailed alternatives analysis was completed for specific architectural layers, discuss those in one of the following subsections.

#+begin_src plantuml :file components.png
actor "user"

package "public-subnet" {

  package "user interface" {
    [data upload and parsing]
    [visualizations]
  }

}

cloud {
  [filestore S3] as S3
}

package "private-subnet" {

  database "Database" {
    [DB] as db
  }

  [API server] as API

}

user ..> [data upload and parsing] : https
user <.. [visualizations] : https

[data upload and parsing] -left-> S3 : file upload

API <-- S3 : file read
API <-> db : CRUD

[data upload and parsing] <--> API : graphql
[visualizations] --> API : graphql
#+end_src
#+RESULTS:
[[file:components.png]]

*** DONE Application Architecture

| Component               | Description                                                                                          | Strategy                                                                                    | Alternatives                                                                                                      |
|-------------------------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------|
| API server              | Application business logic + API endpoints                                                           | Spring Boot + Graphql webserver built around business logic, reuse parts of legacy codebase | Rewrite business logic (parsers) using a hybrid Clojure + Java codebase, use interop with Java whenever possible. |
| Database                | User session data, parser settings, JSON based results for visualizations                            | Use DynamoDB NoSQL database                                                                 | PostgreSQL RDS with [[https://www.postgresql.org/docs/9.4/datatype-json.html][JSONB]] used for storing and querying JSON results                                              |
| Filestore               | Store Input files                                                                                    | Amazon S3                                                                                   | Use IPFS, requires running and maintaing own node                                                                 |
| Data upload and parsing | Web-based interfaces                                                                                 | [[https://reagent-project.github.io/][Reagent]] + reframe codebase                                                                  | _                                                                                                                 |
| Visualizations          | Used to display the outputs on the map in geogroahical coordinatesm, interactive, animated over time | Use reactive library such as [[https://docs.kepler.gl/docs/api-reference][Kepler]]                                                         | [[https://vega.github.io/vega-lite/examples/][Vega]] library                                                                                                      |

# *** TODO Information Architecture
# *** TODO Interface Architecture
*** DONE Technology Architecture <<tech_arch>>

=System Hosting=
System components will be hosted using Amazon Web Services.
The API server along with the Databse component will be hosted on an EC2 instance within the private part of the cloud network, the user and visualization interface will be in a public exposed to the outside world via an Internet Gateway.
Both the server and the user-facing interfaces will be deployed into the QA and PRODUCTION environments as docker images.

=Modes of Operation=
The system will need three environments, the DEV, QA and a PRODUCTION environment.
The development environment will consist of a database, server and interface components running inside a docker containers deployed ot AWS EC2, and an S3 bucket provisioned on the AWS.
The QA environment will be entirely provisioned on the AWS, and be an exact copy of the PRODUCTION environment.

*** DONE Security and Privacy Architecture

=Authentication=
The system will use emails as a method of verifying user identity.
No other user information will be stored.
This approach simplifies authentication concerns and moves them to the email service provider.

Users will simply provide their email addresses, after which an email with a single-use, 15-minute short-lived token will be sent to that address.
An address in the email will bring the user back ti the application, and if the token can be verified user is issued a long-lived 60 day token which the client can save in the browser storage.
Application will track the id field of these tokens.
A [[https://jwt.io/][JWT]] tokens signed with a private key stored in the applications database will be sufficient to cover these use-cases.

=Authorization=
User will authorize to the service by including the issued tokens in the header of every request.

* DONE Analysis of the Proposed System
# ** Impact Analysis
# *** Operational Impacts
# *** Organizational Impacts
# ** Risks
# ** Issues to Resolve
# ** Critical Success Factors for Remainder of Project

=Issues to Resolve=
We can name the following remaining open issues:

- the choice of programming language for the implementation of the server component.
The requirement of code re-use mandates a use of JVM-based language.
Since the original codebas targetted Java 1.6, a web-server writen with the current Long-Term Support Version og the Java programming language and using an enterprise level framework such as [[https://spring.io/projects/spring-boot][Spring Boot]] seems like a natural choice.
However we could also explore a possibility of developing the endpoints, persistance and other parts of the new system in a powerful general-purpose language [[https://clojure.org/][Clojure]], while re-using the old code-base with it's [[https://clojure.org/reference/java_interop][Java interop]].
Clojure is particularly suited for web-based applications and rapid REPL-driven development.

- the choice of the database technology
The object-based nature of the output data produced by the system suggest a use of a noSQL, document-store database such as Amazon [[https://aws.amazon.com/dynamodb/][DynamoDB]].
Since the [[tech_arch][System Hosting]] specifies AWS as a cloud provider, it is worth mentioning that this particular database ties natively into the rest of the infrastructure and provides a virtualy maintainence-free service.
On the other hand it comes with a handfull of tradeoffs (DynamoDB: [[https://blog.yugabyte.com/11-things-you-wish-you-knew-before-starting-with-dynamodb/][strengths and weaknesses]]).

# This particular data-base has some
At the same time a relational database is much more suitable for storing other types of data, such as the user session state, parser settings per analysis and others and are a de-facto standard database for most applications, with different choices of database, and a wide support for all of them.
A PostgreSQL database can be provisioned by the AWS, in the form of their [[https://aws.amazon.com/rds/][RDS]] service.
Lately SQL databases have extended their support for storing and querying JSON based data, introducing a [[https://clojure.org/reference/java_interop][JSONB]] format for storing the data, which can make be leverage for this use-case.

- the choice of the data query and manipulation language for the communication between the user interface and the server
This issue ties to the first one.
The development of the server codebase in the Java language makes using GraphQL as a natural choice, mainly due to the larger number of developer tools.
Going the Clojure route, would make it possible to use am EQL query language implementation like [[https://github.com/wilkerlucio/pathom][Pathom]] possible on both the server and the client component of the system.
Finally with Clojure at the backend it is possible to create a GraphQL API, yet implement a client which uses [[https://wilkerlucio.github.io/pathom/v2/pathom/2.2.0/graphql.html][Pathom GraphQL integration]] for querying it.

=Critical Success Factors=
Implementation of the visualization engine is definitely the sole factor defining the success of the system.
The principle of de-risking would suggest tackling this problem early on and coming up with at least a working prototype of a solution.

* Appendix A: Glossary
- BEAST: software package for phylogenetic analysis with an emphasis on time-scaled trees.
- phylogenetic tree: directed, bifurcating graph depicting ancestral relationship.
